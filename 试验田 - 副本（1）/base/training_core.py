"""
è®­ç»ƒæ ¸å¿ƒæ¨¡å—
æ•´åˆSAMè®­ç»ƒå™¨ã€PANetè®­ç»ƒå™¨ã€æŸå¤±å‡½æ•°å’Œè®­ç»ƒå·¥å…·
"""

import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import cv2
try:
    from torch.amp import GradScaler, autocast
    # æ–°ç‰ˆæœ¬PyTorch
    NEW_AMP_API = True
except ImportError:
    from torch.cuda.amp import GradScaler, autocast
    # æ—§ç‰ˆæœ¬PyTorch
    NEW_AMP_API = False
from torch.utils.checkpoint import checkpoint
import numpy as np
import time
from typing import Dict, List, Tuple, Optional, Any
import json
from tqdm import tqdm
from datetime import datetime
import matplotlib.pyplot as plt
import warnings

# æ£€æµ‹PyTorchç‰ˆæœ¬å¹¶è®¾ç½®å…¼å®¹æ€§
PYTORCH_VERSION = torch.__version__
IS_PYTORCH_2_PLUS = int(PYTORCH_VERSION.split('.')[0]) >= 2

# å…¨å±€æŠ‘åˆ¶PyTorchç›¸å…³çš„FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning, module="torch.cuda.amp")
warnings.filterwarnings("ignore", category=FutureWarning, module="torch.amp")

# TensorBoardæ”¯æŒ
try:
    from torch.utils.tensorboard import SummaryWriter
except ImportError:
    try:
        from tensorboard import SummaryWriter
    except ImportError:
        # å¦‚æœæ²¡æœ‰tensorboardï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„SummaryWriter
        class DummySummaryWriter:
            def __init__(self, *args, **kwargs):
                pass
            def add_scalar(self, *args, **kwargs):
                pass
            def add_image(self, *args, **kwargs):
                pass
            def add_histogram(self, *args, **kwargs):
                pass
            def close(self):
                pass
        SummaryWriter = DummySummaryWriter

# ============================================================================
# æŸå¤±å‡½æ•°
# ============================================================================

class BoundaryDiceLoss(nn.Module):
    """
    è¾¹ç•Œæ„ŸçŸ¥DiceæŸå¤±å‡½æ•° - ä¸“é—¨è§£å†³å¿ƒè„è¶…å£°è¾¹ç•Œæ¨¡ç³Šé—®é¢˜
    åŸºäºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸“ä¸šå®è·µ
    """
    
    def __init__(self, num_classes: int = 5, boundary_weight: float = 0.8):
        super(BoundaryDiceLoss, self).__init__()
        self.num_classes = num_classes
        self.boundary_weight = boundary_weight
    
    def extract_boundaries(self, target: torch.Tensor) -> torch.Tensor:
        """æå–è¾¹ç•ŒåŒºåŸŸï¼ˆä½¿ç”¨å½¢æ€å­¦æ“ä½œï¼Œé¿å…Cannyçš„å¤æ‚æ€§ï¼‰"""
        boundaries = []
        for i in range(target.size(0)):
            # è½¬æ¢ä¸ºnumpyè¿›è¡Œè¾¹ç•Œæå–
            mask = target[i].cpu().numpy().astype(np.uint8)
            
            # å¯¹æ¯ä¸ªç±»åˆ«æå–è¾¹ç•Œ
            boundary = np.zeros_like(mask)
            for class_id in range(1, self.num_classes):  # è·³è¿‡èƒŒæ™¯
                class_mask = (mask == class_id).astype(np.uint8)
                if class_mask.sum() == 0:
                    continue
                
                # ä½¿ç”¨å½¢æ€å­¦æ“ä½œæå–è¾¹ç•Œ
                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                eroded = cv2.erode(class_mask, kernel, iterations=1)
                boundary_region = class_mask - eroded
                boundary[boundary_region > 0] = class_id
            
            boundaries.append(boundary)
        
        return torch.from_numpy(np.array(boundaries)).long().to(target.device)
    
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        Args:
            pred: é¢„æµ‹ç»“æœ (B, C, H, W)
            target: çœŸå®æ ‡ç­¾ (B, H, W)
        """
        # 1. è½¬æ¢ä¸ºone-hotç¼–ç 
        target_onehot = F.one_hot(target, num_classes=self.num_classes).permute(0, 3, 1, 2).float()
        pred_soft = F.softmax(pred, dim=1)
        
        # 2. æå–è¾¹ç•ŒåŒºåŸŸ
        boundary_mask = self.extract_boundaries(target)  # (B, H, W)
        
        # 3. åˆ›å»ºè¾¹ç•Œæƒé‡å›¾
        boundary_weight_map = torch.zeros_like(target).float()
        for class_id in range(1, self.num_classes):
            boundary_weight_map[boundary_mask == class_id] = 1.0
        
        # 4. è®¡ç®—æ ‡å‡†Dice
        intersection_std = (pred_soft * target_onehot).sum(dim=(2, 3))
        union_std = pred_soft.sum(dim=(2, 3)) + target_onehot.sum(dim=(2, 3))
        dice_std = (2.0 * intersection_std + 1e-6) / (union_std + 1e-6)
        
        # 5. è®¡ç®—è¾¹ç•ŒDiceï¼ˆä»…åœ¨è¾¹ç•ŒåŒºåŸŸï¼‰
        boundary_weight_expanded = boundary_weight_map.unsqueeze(1).expand_as(pred_soft)
        pred_boundary = pred_soft * boundary_weight_expanded
        target_boundary = target_onehot * boundary_weight_expanded
        
        intersection_boundary = (pred_boundary * target_boundary).sum(dim=(2, 3))
        union_boundary = pred_boundary.sum(dim=(2, 3)) + target_boundary.sum(dim=(2, 3))
        dice_boundary = (2.0 * intersection_boundary + 1e-6) / (union_boundary + 1e-6)
        
        # 6. ç»„åˆæŸå¤±ï¼šé‡ç‚¹å…³æ³¨è¾¹ç•Œ
        dice_loss_std = 1 - dice_std.mean()
        dice_loss_boundary = 1 - dice_boundary.mean()
        
        # è¾¹ç•ŒæŸå¤±æƒé‡æ›´é«˜
        total_loss = (1 - self.boundary_weight) * dice_loss_std + self.boundary_weight * dice_loss_boundary
        
        return total_loss

class FocalDiceLoss(nn.Module):
    """
    Focal DiceæŸå¤±å‡½æ•°
    """
    
    def __init__(self, 
                 num_classes: int = 5,
                 focal_alpha: float = 0.25,
                 focal_gamma: float = 2.0,
                 dice_weight: float = 0.7,
                 ce_weight: float = 0.3,
                 class_weights: Optional[torch.Tensor] = None,
                 smooth: float = 1e-6,
                 use_hard_mining: bool = True,
                 mining_threshold: float = 0.3):
        """
        åˆå§‹åŒ–Focal DiceæŸå¤±
        
        Args:
            num_classes: ç±»åˆ«æ•°é‡
            focal_alpha: Focal lossçš„alphaå‚æ•°
            focal_gamma: Focal lossçš„gammaå‚æ•°
            dice_weight: DiceæŸå¤±çš„æƒé‡
            ce_weight: äº¤å‰ç†µæŸå¤±çš„æƒé‡
            class_weights: ç±»åˆ«æƒé‡
            smooth: å¹³æ»‘å› å­
            use_hard_mining: æ˜¯å¦ä½¿ç”¨å›°éš¾æ ·æœ¬æŒ–æ˜
            mining_threshold: å›°éš¾æ ·æœ¬æŒ–æ˜é˜ˆå€¼
        """
        super(FocalDiceLoss, self).__init__()
        self.num_classes = num_classes
        self.focal_alpha = focal_alpha
        self.focal_gamma = focal_gamma
        self.dice_weight = dice_weight
        self.ce_weight = ce_weight
        self.class_weights = class_weights
        self.smooth = smooth
        self.use_hard_mining = use_hard_mining
        self.mining_threshold = mining_threshold
              
        if class_weights is not None:
            self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)
        else:
            self.ce_loss = nn.CrossEntropyLoss()
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            inputs: é¢„æµ‹ç»“æœ [B, C, H, W]
            targets: çœŸå®æ ‡ç­¾ [B, H, W]
        
        Returns:
            æŸå¤±å€¼
        """
        # ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
        if len(inputs.shape) == 4 and inputs.shape[1] != self.num_classes:
            inputs = inputs.permute(0, 3, 1, 2)
        
        # è®¡ç®—äº¤å‰ç†µ
        ce_loss = self.ce_loss(inputs, targets)
        
        # è®¡ç®—Focal Loss
        focal_loss = self._focal_loss(inputs, targets)
        
        # è®¡ç®—Dice Loss
        dice_loss = self._dice_loss(inputs, targets)
        
        # ç»„åˆæŸå¤±
        total_loss = (self.ce_weight * ce_loss + 
                     self.focal_alpha * focal_loss + 
                     self.dice_weight * dice_loss)
        
        return total_loss
    
    def _focal_loss(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—Focal Loss"""
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        
        # è®¡ç®—æ¦‚ç‡
        pt = torch.exp(-ce_loss)
        
        # è®¡ç®—focalæƒé‡
        focal_weight = self.focal_alpha * (1 - pt) ** self.focal_gamma
        
        # åº”ç”¨å›°éš¾æ ·æœ¬æŒ–æ˜
        if self.use_hard_mining:
            # é€‰æ‹©å›°éš¾æ ·æœ¬
            hard_mask = ce_loss > self.mining_threshold
            focal_weight = focal_weight * hard_mask.float()
        
        return (focal_weight * ce_loss).mean()
    
    def _dice_loss(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—Dice Loss - æ•°å€¼ç¨³å®šç‰ˆæœ¬
        """
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰æ•ˆ
        if torch.isnan(inputs).any() or torch.isinf(inputs).any():
            print("âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆçš„è¾“å…¥logitsï¼Œä½¿ç”¨å¤‡ç”¨è®¡ç®—")
            return torch.tensor(1.0, device=inputs.device, requires_grad=True)
        
        # è½¬æ¢ä¸ºone-hotç¼–ç 
        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes)
        targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()
        
        # ä½¿ç”¨æ•°å€¼ç¨³å®šçš„softmax
        inputs_soft = F.softmax(inputs, dim=1)
        
        # æ£€æŸ¥softmaxç»“æœ
        if torch.isnan(inputs_soft).any():
            print("âš ï¸ softmaxäº§ç”ŸNaNï¼Œä½¿ç”¨å¤‡ç”¨è®¡ç®—")
            return torch.tensor(1.0, device=inputs.device, requires_grad=True)
        
        # è®¡ç®—Diceç³»æ•° - å¢å¼ºæ•°å€¼ç¨³å®šæ€§
        intersection = (inputs_soft * targets_one_hot).sum(dim=(2, 3))
        union = inputs_soft.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3))
        
        # å¢å¤§smoothé¡¹é˜²æ­¢é™¤é›¶
        dice = (2.0 * intersection + self.smooth * 10) / (union + self.smooth * 10)
        
        # æ£€æŸ¥diceç»“æœ
        if torch.isnan(dice).any():
            print("âš ï¸ diceè®¡ç®—äº§ç”ŸNaNï¼Œä½¿ç”¨å¤‡ç”¨å€¼")
            return torch.tensor(0.5, device=inputs.device, requires_grad=True)
        
        dice_loss = 1 - dice.mean()
        
        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(dice_loss) or torch.isinf(dice_loss):
            return torch.tensor(0.5, device=inputs.device, requires_grad=True)
        
        return dice_loss

class CombinedLoss(nn.Module):
    """
    ç»„åˆæŸå¤±å‡½æ•°
    ç»“åˆå¤šç§æŸå¤±å‡½æ•°ï¼Œæä¾›æ›´ç¨³å®šçš„è®­ç»ƒ
    """
    
    def __init__(self,
                 num_classes: int = 5,
                 ce_weight: float = 0.4,
                 dice_weight: float = 0.4,
                 focal_weight: float = 0.2,
                 class_weights: Optional[torch.Tensor] = None,
                 smooth: float = 1e-6):
        """
        åˆå§‹åŒ–ç»„åˆæŸå¤±
        
        Args:
            num_classes: ç±»åˆ«æ•°é‡
            ce_weight: äº¤å‰ç†µæŸå¤±æƒé‡
            dice_weight: DiceæŸå¤±æƒé‡
            focal_weight: FocalæŸå¤±æƒé‡
            class_weights: ç±»åˆ«æƒé‡
            smooth: å¹³æ»‘å› å­
        """
        super(CombinedLoss, self).__init__()
        self.num_classes = num_classes
        self.ce_weight = ce_weight
        self.dice_weight = dice_weight
        self.focal_weight = focal_weight
        self.smooth = smooth
        
        if class_weights is not None:
            self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)
        else:
            self.ce_loss = nn.CrossEntropyLoss()
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            inputs: é¢„æµ‹ç»“æœ [B, C, H, W]
            targets: çœŸå®æ ‡ç­¾ [B, H, W]
        
        Returns:
            æŸå¤±å€¼
        """
        ce_loss = self.ce_loss(inputs, targets)
        
        # DiceæŸå¤±
        dice_loss = self._dice_loss(inputs, targets)
        
        # FocalæŸå¤±
        focal_loss = self._focal_loss(inputs, targets)
        
        # ç»„åˆæŸå¤±
        total_loss = (self.ce_weight * ce_loss + 
                     self.dice_weight * dice_loss + 
                     self.focal_weight * focal_loss)
        
        return total_loss
    
    def _dice_loss(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—Dice Loss - æ•°å€¼ç¨³å®šç‰ˆæœ¬
        """
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰æ•ˆ
        if torch.isnan(inputs).any() or torch.isinf(inputs).any():
            print("âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆçš„è¾“å…¥logitsï¼Œä½¿ç”¨å¤‡ç”¨è®¡ç®—")
            return torch.tensor(1.0, device=inputs.device, requires_grad=True)
        
        # è½¬æ¢ä¸ºone-hotç¼–ç 
        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes)
        targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()
        
        # ä½¿ç”¨æ•°å€¼ç¨³å®šçš„softmax
        inputs_soft = F.softmax(inputs, dim=1)
        
        # æ£€æŸ¥softmaxç»“æœ
        if torch.isnan(inputs_soft).any():
            print("âš ï¸ softmaxäº§ç”ŸNaNï¼Œä½¿ç”¨å¤‡ç”¨è®¡ç®—")
            return torch.tensor(1.0, device=inputs.device, requires_grad=True)
        
        # è®¡ç®—Diceç³»æ•° - å¢å¼ºæ•°å€¼ç¨³å®šæ€§
        intersection = (inputs_soft * targets_one_hot).sum(dim=(2, 3))
        union = inputs_soft.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3))
        
        # å¢å¤§smoothé¡¹é˜²æ­¢é™¤é›¶
        dice = (2.0 * intersection + self.smooth * 10) / (union + self.smooth * 10)
        
        # æ£€æŸ¥diceç»“æœ
        if torch.isnan(dice).any():
            print("âš ï¸ diceè®¡ç®—äº§ç”ŸNaNï¼Œä½¿ç”¨å¤‡ç”¨å€¼")
            return torch.tensor(0.5, device=inputs.device, requires_grad=True)
        
        dice_loss = 1 - dice.mean()
        
        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(dice_loss) or torch.isinf(dice_loss):
            return torch.tensor(0.5, device=inputs.device, requires_grad=True)
        
        return dice_loss
    
    def _focal_loss(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—Focal Loss"""
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        
        # è®¡ç®—æ¦‚ç‡
        pt = torch.exp(-ce_loss)
        
        # è®¡ç®—focalæƒé‡
        focal_weight = (1 - pt) ** 2
        
        return (focal_weight * ce_loss).mean()

class DiversityEnhancedLoss(nn.Module):
    """
    å¤šæ ·æ€§å¢å¼ºæŸå¤±å‡½æ•° - è§£å†³å•ä¸€ç±»åˆ«é¢„æµ‹é—®é¢˜
    """
    
    def __init__(self, main_loss, diversity_weight: float = 0.1):
        super().__init__()
        self.main_loss = main_loss
        self.diversity_weight = diversity_weight
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            inputs: é¢„æµ‹ç»“æœ [B, C, H, W]
            targets: çœŸå®æ ‡ç­¾ [B, H, W]
        
        Returns:
            ç»„åˆæŸå¤±å€¼
        """
        # ä¸»æŸå¤±
        main_loss = self.main_loss(inputs, targets)
        
        # å¤šæ ·æ€§æ­£åˆ™åŒ–æŸå¤±
        diversity_loss = self._diversity_regularization(inputs)
        
        # ç»„åˆæŸå¤±
        total_loss = main_loss + self.diversity_weight * diversity_loss
        
        return total_loss
    
    def _diversity_regularization(self, inputs: torch.Tensor) -> torch.Tensor:
        """
        å¤šæ ·æ€§æ­£åˆ™åŒ– - æƒ©ç½šå•ä¸€ç±»åˆ«é¢„æµ‹
        """
        # è®¡ç®—é¢„æµ‹æ¦‚ç‡
        probs = F.softmax(inputs, dim=1)  # [B, C, H, W]
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡é¢„æµ‹æ¦‚ç‡
        class_probs = probs.mean(dim=(0, 2, 3))  # [C]
        
        # ç›®æ ‡ï¼šæ¯ä¸ªç±»åˆ«éƒ½åº”è¯¥æœ‰ä¸€å®šçš„é¢„æµ‹æ¦‚ç‡
        target_prob = 1.0 / inputs.shape[1]  # å‡åŒ€åˆ†å¸ƒ
        
        # ä½¿ç”¨KLæ•£åº¦æƒ©ç½šåç¦»å‡åŒ€åˆ†å¸ƒ
        uniform_dist = torch.full_like(class_probs, target_prob)
        kl_div = F.kl_div(
            torch.log(class_probs + 1e-8), 
            uniform_dist, 
            reduction='sum'
        )
        
        # é¢å¤–çš„ç†µæ­£åˆ™åŒ– - é¼“åŠ±é¢„æµ‹çš„ä¸ç¡®å®šæ€§
        entropy_loss = -(probs * torch.log(probs + 1e-8)).sum(dim=1).mean()
        entropy_regularization = -entropy_loss  # è´Ÿå·å› ä¸ºæˆ‘ä»¬æƒ³æœ€å¤§åŒ–ç†µ
        
        return kl_div + 0.1 * entropy_regularization

# ============================================================================
# è®­ç»ƒå·¥å…·ç±»
# ============================================================================

class TrainingUtils:
    """
    è®­ç»ƒå·¥å…·ç±»
    æä¾›è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§è¾…åŠ©åŠŸèƒ½
    """
    
    def __init__(self):
        """åˆå§‹åŒ–è®­ç»ƒå·¥å…·"""
        pass
    
    @staticmethod
    def save_checkpoint(model: nn.Module,
                       optimizer,
                       scheduler,
                       epoch: int,
                       loss: float,
                       save_path: str,
                       **kwargs):
        """
        ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹
        
        Args:
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
            epoch: å½“å‰epoch
            loss: å½“å‰æŸå¤±
            save_path: ä¿å­˜è·¯å¾„
            **kwargs: å…¶ä»–éœ€è¦ä¿å­˜çš„ä¿¡æ¯
        """
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'loss': loss,
            **kwargs
        }
        
        torch.save(checkpoint, save_path)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")
    
    @staticmethod
    def load_checkpoint(model: nn.Module,
                       optimizer=None,
                       scheduler=None,
                       checkpoint_path: str = None,
                       device: str = 'cuda'):
        """
        åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹
        
        Args:
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
            device: è®¾å¤‡
        
        Returns:
            åŠ è½½çš„epochå’ŒæŸå¤±
        """
        if not os.path.exists(checkpoint_path):
            print(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return 0, float('inf')
        
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
        if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        epoch = checkpoint.get('epoch', 0)
        loss = checkpoint.get('loss', float('inf'))
        
        print(f"æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}, epoch={epoch}, loss={loss:.4f}")
        return epoch, loss
    
    @staticmethod
    def calculate_metrics(predictions: torch.Tensor, 
                         targets: torch.Tensor,
                         num_classes: int = 5) -> Dict[str, float]:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡ - ç»Ÿä¸€ä½¿ç”¨argmaxæ–¹æ³•
        
        Args:
            predictions: é¢„æµ‹ç»“æœ [B, H, W] æˆ– [B, C, H, W] æˆ– [B, 1, H, W]
            targets: çœŸå®æ ‡ç­¾ [B, H, W]
            num_classes: ç±»åˆ«æ•°é‡
        
        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        # è½¬æ¢ä¸ºnumpy
        if isinstance(predictions, torch.Tensor):
            predictions = predictions.detach().cpu().numpy()
        if isinstance(targets, torch.Tensor):
            targets = targets.detach().cpu().numpy()
        
        # å¤„ç†ä¸åŒçš„è¾“å…¥æ ¼å¼
        if len(predictions.shape) == 4:
            if predictions.shape[1] == 1:
                # [B, 1, H, W] -> [B, H, W]
                pred_classes = predictions.squeeze(1).astype(int)
            else:
                # [B, C, H, W] -> [B, H, W]
                pred_classes = np.argmax(predictions, axis=1)
        else:
            # [B, H, W]
            pred_classes = predictions.astype(int)
        
        # è®¡ç®—Diceç³»æ•°
        dice_scores = []
        for i in range(num_classes):
            pred_mask = (pred_classes == i)
            target_mask = (targets == i)
            
            if target_mask.sum() > 0:
                intersection = (pred_mask & target_mask).sum()
                union = pred_mask.sum() + target_mask.sum()
                dice = 2.0 * intersection / (union + 1e-8)
                dice_scores.append(dice)
            else:
                dice_scores.append(0.0)
        
        # è®¡ç®—å¹³å‡Dice
        mean_dice = np.mean(dice_scores)
        
        # è®¡ç®—åƒç´ å‡†ç¡®ç‡
        pixel_accuracy = (pred_classes == targets).mean()
        
        return {
            'mean_dice': mean_dice,
            'pixel_accuracy': pixel_accuracy,
            'dice_per_class': dice_scores
        }
    
    @staticmethod
    def plot_training_curves(train_losses: List[float],
                           val_losses: List[float],
                           train_metrics: List[Dict[str, float]] = None,
                           val_metrics: List[Dict[str, float]] = None,
                           save_path: str = None):
        """
        ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        
        Args:
            train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
            val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
            train_metrics: è®­ç»ƒæŒ‡æ ‡åˆ—è¡¨
            val_metrics: éªŒè¯æŒ‡æ ‡åˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(train_losses, label='Train Loss')
        axes[0, 0].plot(val_losses, label='Val Loss')
        axes[0, 0].set_title('Training and Validation Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Diceç³»æ•°æ›²çº¿
        if train_metrics and val_metrics:
            train_dice = [m['mean_dice'] for m in train_metrics]
            val_dice = [m['mean_dice'] for m in val_metrics]
            
            axes[0, 1].plot(train_dice, label='Train Dice')
            axes[0, 1].plot(val_dice, label='Val Dice')
            axes[0, 1].set_title('Mean Dice Score')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Dice Score')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
        
        # åƒç´ å‡†ç¡®ç‡æ›²çº¿
        if train_metrics and val_metrics:
            train_acc = [m['pixel_accuracy'] for m in train_metrics]
            val_acc = [m['pixel_accuracy'] for m in val_metrics]
            
            axes[1, 0].plot(train_acc, label='Train Accuracy')
            axes[1, 0].plot(val_acc, label='Val Accuracy')
            axes[1, 0].set_title('Pixel Accuracy')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Accuracy')
            axes[1, 0].legend()
            axes[1, 0].grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        axes[1, 1].text(0.5, 0.5, 'Learning Rate Curve\n(Not Available)', 
                       ha='center', va='center', transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('Learning Rate')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")
        
        plt.show()

# ============================================================================
# SAMè®­ç»ƒå™¨
# ============================================================================

class SAMTrainer:
    """
    SAMæ¨¡å‹è®­ç»ƒå™¨
    å®ç°SAMæ¨¡å‹çš„è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•åŠŸèƒ½
    """
    
    def __init__(self,
                 model: nn.Module,
                 train_loader: DataLoader,
                 val_loader: DataLoader = None,
                 test_loader: DataLoader = None,
                 device: str = 'cuda',
                 learning_rate: float = 1e-4,
                 weight_decay: float = 1e-4,
                 num_epochs: int = 100,
                 save_dir: str = 'shared/checkpoints',
                 use_amp: bool = True,
                 use_checkpoint: bool = False,
                 accumulation_steps: int = 1,
                 early_stopping_patience: int = 5,
                 criterion: nn.Module = None):
        """
        åˆå§‹åŒ–SAMè®­ç»ƒå™¨
        
        Args:
            model: SAMæ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            device: è®¾å¤‡
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            num_epochs: è®­ç»ƒè½®æ•°
            save_dir: ä¿å­˜ç›®å½•
            use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            use_checkpoint: æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.device = device
        self.num_epochs = num_epochs
        self.save_dir = save_dir
        self.use_amp = use_amp
        self.use_checkpoint = use_checkpoint
        self.accumulation_steps = accumulation_steps
        
        # æ—©åœæœºåˆ¶
        self.early_stopping_patience = early_stopping_patience
        self.patience_counter = 0
        self.best_epoch = 0
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=num_epochs,
            eta_min=learning_rate * 0.01
        )
        
        # åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒ
        if use_amp:
            if NEW_AMP_API:
                self.scaler = GradScaler('cuda')
            else:
                self.scaler = GradScaler()
        else:
            self.scaler = None
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        if criterion is not None:
            self.criterion = criterion.to(device)
        else:
            self.criterion = self._create_loss_function()
        
        # åˆå§‹åŒ–TensorBoard - ç¡®ä¿è¾“å‡ºåˆ°sharedç›®å½•
        if not save_dir.startswith('shared/'):
            save_dir = f'shared/{save_dir}'
        self.writer = SummaryWriter(os.path.join(save_dir, 'logs'))
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.best_val_dice = 0.0
        
        # é™é»˜åˆå§‹åŒ–å®Œæˆ
    
    def _create_loss_function(self):
        """åˆ›å»ºæŸå¤±å‡½æ•° - å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç»„åˆæŸå¤±"""
        class_weights = self._calculate_class_weights()
        
        # **å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•°(Dice+CE)**
        # é™é»˜ä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•°(Dice+CE)
        
        # æ ¹æ®æ¨¡å‹ç±»åˆ«æ•°åŠ¨æ€åˆ›å»ºæŸå¤±å‡½æ•°
        if hasattr(self.model, 'num_classes'):
            num_classes = self.model.num_classes
        else:
            num_classes = 5
        
        # åˆ›å»ºç»„åˆæŸå¤±å‡½æ•°
        return CombinedLoss(
            num_classes=num_classes,
            class_weights=class_weights,
            ce_weight=0.5,  # äº¤å‰ç†µæƒé‡
            dice_weight=0.5,  # Diceæƒé‡
            focal_weight=0.0  # æš‚æ—¶ä¸ä½¿ç”¨Focal
        )
        
        # å¯¹äº2ç±»å’Œ3ç±»ï¼Œä¸éœ€è¦å¤šæ ·æ€§æŸå¤±ï¼ˆé—®é¢˜è¾ƒç®€å•ï¼‰
        if num_classes <= 3:
            return main_loss
        else:
            # åªæœ‰5ç±»æ—¶æ‰ä½¿ç”¨å¤šæ ·æ€§å¢å¼º
            return DiversityEnhancedLoss(main_loss, diversity_weight=0.5)
    
    def _calculate_class_weights(self):
        """è®¡ç®—ç±»åˆ«æƒé‡ - æ ¹æ®å½“å‰æ¨¡å‹çš„ç±»åˆ«æ•°åŠ¨æ€è°ƒæ•´"""
        
        # æ ¹æ®æ¨¡å‹çš„å®é™…ç±»åˆ«æ•°è®¾ç½®æƒé‡
        if hasattr(self.model, 'num_classes'):
            num_classes = self.model.num_classes
        else:
            num_classes = 5  # é»˜è®¤5ç±»
        
        if num_classes == 2:
            # 2ç±»ï¼šèƒŒæ™¯ vs å¿ƒè„ - åˆç†æƒé‡
            weights = torch.tensor([0.4, 1.5], dtype=torch.float32)
            class_names = ['èƒŒæ™¯', 'å¿ƒè„']
        elif num_classes == 3:
            # 3ç±»ï¼šèƒŒæ™¯ + å·¦å¿ƒ + å³å¿ƒ
            weights = torch.tensor([0.4, 1.5, 1.5], dtype=torch.float32)
            class_names = ['èƒŒæ™¯', 'å·¦å¿ƒ', 'å³å¿ƒ']
        else:
            # 5ç±»ï¼šå®Œæ•´åˆ†å‰²
            weights = torch.tensor([
                0.5,   # èƒŒæ™¯ï¼šé€‚ä¸­æƒé‡
                2.0,   # å·¦å¿ƒå®¤ï¼šæé«˜æƒé‡ä½†ä¸è¿‡åº¦
                2.5,   # å³å¿ƒå®¤ï¼šæœ€é«˜æƒé‡ï¼ˆæœ€ç¨€å°‘ç±»åˆ«ï¼‰
                2.0,   # å·¦å¿ƒæˆ¿ï¼šæé«˜æƒé‡ä½†ä¸è¿‡åº¦
                1.8    # å³å¿ƒæˆ¿ï¼šé€‚ä¸­æé«˜æƒé‡
            ], dtype=torch.float32)
            class_names = ['èƒŒæ™¯', 'å·¦å¿ƒå®¤', 'å³å¿ƒå®¤', 'å·¦å¿ƒæˆ¿', 'å³å¿ƒæˆ¿']
        
        # é™é»˜è®¾ç½®ç±»åˆ«æƒé‡
        
        return weights.to(self.device)
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        total_samples = 0
        
        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}')
        
        for batch_idx, batch in enumerate(pbar):
            # å‡†å¤‡æ•°æ®
            if isinstance(batch, dict):
                images = batch['image'].to(self.device)
                labels = batch['label'].to(self.device)
            else:
                images, labels = batch
                images = images.to(self.device)
                labels = labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            if self.use_amp and self.scaler:
                if NEW_AMP_API:
                    with autocast(device_type='cuda'):
                        # MedSAMä¼˜åŒ–ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤promptç”Ÿæˆ
                        # MedSAMä¼šè‡ªåŠ¨å¤„ç†promptç”Ÿæˆï¼Œæ— éœ€æ‰‹åŠ¨åˆ›å»ºè™šæ‹Ÿprompts
                        batch_size = images.shape[0]
                        points = None  # MedSAMä¼šè‡ªåŠ¨ç”Ÿæˆé»˜è®¤prompts
                        boxes = None
                        masks = None
                        
                        if self.use_checkpoint:
                            outputs = checkpoint(self.model, images, points, boxes, masks)
                        else:
                            outputs = self.model(images, points, boxes, masks)
                        
                        # MedSAMä¼˜åŒ–ï¼šå¤„ç†æ–°çš„è¾“å‡ºæ ¼å¼
                        if isinstance(outputs, dict):
                            # ä¼˜å…ˆä½¿ç”¨cardiac_logitsï¼Œè¿™æ˜¯MedSAMçš„ä¸»è¦è¾“å‡º
                            outputs = outputs.get('cardiac_logits', outputs.get('predictions', outputs.get('sam_masks', outputs)))
                        
                        loss = self.criterion(outputs, labels)
                        loss = loss / self.accumulation_steps
                else:
                    with autocast():
                        # MedSAMä¼˜åŒ–ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤promptç”Ÿæˆ
                        # MedSAMä¼šè‡ªåŠ¨å¤„ç†promptç”Ÿæˆï¼Œæ— éœ€æ‰‹åŠ¨åˆ›å»ºè™šæ‹Ÿprompts
                        batch_size = images.shape[0]
                        points = None  # MedSAMä¼šè‡ªåŠ¨ç”Ÿæˆé»˜è®¤prompts
                        boxes = None
                        masks = None
                        
                        if self.use_checkpoint:
                            outputs = checkpoint(self.model, images, points, boxes, masks)
                        else:
                            outputs = self.model(images, points, boxes, masks)
                        
                        # MedSAMä¼˜åŒ–ï¼šå¤„ç†æ–°çš„è¾“å‡ºæ ¼å¼
                        if isinstance(outputs, dict):
                            # ä¼˜å…ˆä½¿ç”¨cardiac_logitsï¼Œè¿™æ˜¯MedSAMçš„ä¸»è¦è¾“å‡º
                            outputs = outputs.get('cardiac_logits', outputs.get('predictions', outputs.get('sam_masks', outputs)))
                        
                        loss = self.criterion(outputs, labels)
                        loss = loss / self.accumulation_steps
                
                # åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
                
                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % self.accumulation_steps == 0:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
            else:
                # MedSAMä¼˜åŒ–ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤promptç”Ÿæˆ
                batch_size = images.shape[0]
                points = None  # MedSAMä¼šè‡ªåŠ¨ç”Ÿæˆé»˜è®¤prompts
                boxes = None
                masks = None
                
                if self.use_checkpoint:
                    outputs = checkpoint(self.model, images, points, boxes, masks)
                else:
                    outputs = self.model(images, points, boxes, masks)
                
                # MedSAMä¼˜åŒ–ï¼šå¤„ç†æ–°çš„è¾“å‡ºæ ¼å¼
                if isinstance(outputs, dict):
                    # ä¼˜å…ˆä½¿ç”¨cardiac_logitsï¼Œè¿™æ˜¯MedSAMçš„ä¸»è¦è¾“å‡º
                    outputs = outputs.get('cardiac_logits', outputs.get('predictions', outputs.get('sam_masks', outputs)))
                
                loss = self.criterion(outputs, labels)
                loss = loss / self.accumulation_steps
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % self.accumulation_steps == 0:
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                
                # æ›´é¢‘ç¹çš„å†…å­˜æ¸…ç†
                del outputs, loss
                if batch_idx % 5 == 0:  # æ¯5ä¸ªæ‰¹æ¬¡æ¸…ç†ä¸€æ¬¡
                    torch.cuda.empty_cache()
                    
                # é™åˆ¶è®­ç»ƒé€Ÿåº¦ï¼Œç»™GPUå–˜æ¯æ—¶é—´
                if batch_idx % 20 == 0:
                    import time
                    time.sleep(0.1)
            
            # æ›´æ–°ç»Ÿè®¡ - ä¿®å¤NaNé—®é¢˜
            current_loss = loss.item() * self.accumulation_steps
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ - å¢å¼ºæ£€æŸ¥
            if torch.isnan(loss) or torch.isinf(loss) or current_loss > 100:
                print(f"âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸æŸå¤±å€¼: {current_loss}, è·³è¿‡æ­¤batch")
                # æ¸…ç†å¼‚å¸¸çŠ¶æ€
                self.optimizer.zero_grad()
                torch.cuda.empty_cache()
                continue
            
            total_loss += current_loss
            total_samples += images.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡ - ä¿®å¤NaNæ˜¾ç¤º
            avg_loss = total_loss / (batch_idx + 1) if (batch_idx + 1) > 0 else 0
            pbar.set_postfix({
                'loss': f'{current_loss:.4f}',
                'avg_loss': f'{avg_loss:.4f}'
            })
            
            # æ·»åŠ æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            if (batch_idx + 1) % self.accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / len(self.train_loader)
        
        return {'train_loss': avg_loss}
    
    def validate_epoch(self, epoch: int) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        if self.val_loader is None:
            return {}
        
        self.model.eval()
        total_loss = 0.0
        total_metrics = {'mean_dice': 0.0, 'pixel_accuracy': 0.0}
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validation'):
                # å‡†å¤‡æ•°æ®
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                    labels = batch['label'].to(self.device)
                else:
                    images, labels = batch
                    images = images.to(self.device)
                    labels = labels.to(self.device)
                
                # å‰å‘ä¼ æ’­
                if self.use_amp and self.scaler:
                    if NEW_AMP_API:
                        with autocast(device_type='cuda'):
                            # MedSAMä¼˜åŒ–ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤promptç”Ÿæˆ
                            batch_size = images.shape[0]
                            points = None  # MedSAMä¼šè‡ªåŠ¨ç”Ÿæˆé»˜è®¤prompts
                            boxes = None
                            masks = None
                            
                            outputs = self.model(images, points, boxes, masks)
                            
                            # å¤„ç†å­—å…¸è¾“å‡º
                            if isinstance(outputs, dict):
                                outputs = outputs.get('cardiac_logits', outputs.get('sam_masks', outputs))
                            
                            loss = self.criterion(outputs, labels)
                    else:
                        with autocast():
                            # MedSAMä¼˜åŒ–ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤promptç”Ÿæˆ
                            batch_size = images.shape[0]
                            points = None  # MedSAMä¼šè‡ªåŠ¨ç”Ÿæˆé»˜è®¤prompts
                            boxes = None
                            masks = None
                            
                            outputs = self.model(images, points, boxes, masks)
                            
                            # å¤„ç†å­—å…¸è¾“å‡º
                            if isinstance(outputs, dict):
                                outputs = outputs.get('cardiac_logits', outputs.get('sam_masks', outputs))
                            
                            loss = self.criterion(outputs, labels)
                else:
                    # MedSAMä¼˜åŒ–ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤promptç”Ÿæˆ
                    batch_size = images.shape[0]
                    points = None  # MedSAMä¼šè‡ªåŠ¨ç”Ÿæˆé»˜è®¤prompts
                    boxes = None
                    masks = None
                    
                    outputs = self.model(images, points, boxes, masks)
                    
                    # å¤„ç†å­—å…¸è¾“å‡º
                    if isinstance(outputs, dict):
                        outputs = outputs.get('cardiac_logits', outputs.get('sam_masks', outputs))
                    
                    loss = self.criterion(outputs, labels)
                
                # æ›´æ–°ç»Ÿè®¡
                total_loss += loss.item()
                
                # è®¡ç®—æŒ‡æ ‡ - ä½¿ç”¨ä¸æ¨ç†ä¸€è‡´çš„argmaxæ–¹æ³•
                pred_classes = torch.argmax(outputs, dim=1)  # ç›´æ¥ä½¿ç”¨argmax
                metrics = TrainingUtils.calculate_metrics(pred_classes.unsqueeze(1), labels)
                for key, value in metrics.items():
                    if key in total_metrics:
                        total_metrics[key] += value
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = total_loss / len(self.val_loader)
        avg_metrics = {key: value / len(self.val_loader) for key, value in total_metrics.items()}
        
        return {'val_loss': avg_loss, **avg_metrics}
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        # å¼€å§‹è®­ç»ƒ
        
        # è®°å½•è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        val_dices = []
        
        for epoch in range(self.num_epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(epoch)
            
            # éªŒè¯
            val_metrics = self.validate_epoch(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # è®°å½•æŒ‡æ ‡
            all_metrics = {**train_metrics, **val_metrics}
            for key, value in all_metrics.items():
                self.writer.add_scalar(key, value, epoch)
            
            # è®°å½•å†å²
            train_losses.append(train_metrics.get('train_loss', 0))
            val_losses.append(val_metrics.get('val_loss', 0))
            val_dices.append(val_metrics.get('mean_dice', 0))
            
            # æ‰“å°è¿›åº¦
            print(f"Epoch {epoch+1}/{self.num_epochs}: "
                  f"Train Loss: {train_metrics.get('train_loss', 0):.4f}, "
                  f"Val Loss: {val_metrics.get('val_loss', 0):.4f}, "
                  f"Val Dice: {val_metrics.get('mean_dice', 0):.4f}")
            
            # æ—©åœæœºåˆ¶æ£€æŸ¥
            current_dice = val_metrics.get('mean_dice', 0)
            if current_dice > self.best_val_dice:
                self.best_val_dice = current_dice
                self.best_epoch = epoch
                self.patience_counter = 0
                self.save_checkpoint(is_best=True, suffix='_best_dice')
                print(f"ğŸ¯ æ–°çš„æœ€ä½³Dice: {current_dice:.4f}")
            else:
                self.patience_counter += 1
                print(f"â³ æ—©åœè®¡æ•°å™¨: {self.patience_counter}/{self.early_stopping_patience}")
            
            # ä¿å­˜æœ€ä½³æŸå¤±æ¨¡å‹
            if val_metrics.get('val_loss', float('inf')) < self.best_val_loss:
                self.best_val_loss = val_metrics.get('val_loss', float('inf'))
                self.save_checkpoint(is_best=True)
            
            # æ—©åœæ£€æŸ¥
            if self.patience_counter >= self.early_stopping_patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘! æœ€ä½³Dice: {self.best_val_dice:.4f} (ç¬¬{self.best_epoch+1}è½®)")
                print(f"   å·²è¿ç»­{self.patience_counter}è½®æ— æ”¹å–„ï¼Œåœæ­¢è®­ç»ƒ")
                break
        
        print("SAMæ¨¡å‹è®­ç»ƒå®Œæˆ!")
        self.writer.close()
        
        # è¿”å›è®­ç»ƒç»“æœ
        return {
            'final_train_loss': train_losses[-1] if train_losses else 0,
            'final_val_loss': val_losses[-1] if val_losses else 0,
            'final_val_dice': val_dices[-1] if val_dices else 0,
            'best_val_loss': self.best_val_loss,
            'best_val_dice': self.best_val_dice,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'val_dices': val_dices
        }
    
    def save_checkpoint(self, is_best: bool = False, suffix: str = ''):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = os.path.join(
            self.save_dir, 
            f'sam_checkpoint_epoch_{self.current_epoch}{suffix}.pth'
        )
        
        TrainingUtils.save_checkpoint(
            model=self.model,
            optimizer=self.optimizer,
            scheduler=self.scheduler,
            epoch=self.current_epoch,
            loss=self.best_val_loss,
            save_path=checkpoint_path,
            best_val_dice=self.best_val_dice
        )

# ============================================================================
# PANetè®­ç»ƒå™¨
# ============================================================================

class PANetTrainer:
    """
    PANetæ¨¡å‹è®­ç»ƒå™¨
    å®ç°PANetæ¨¡å‹çš„è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•åŠŸèƒ½
    """
    
    def __init__(self,
                 model: nn.Module,
                 train_loader: DataLoader,
                 val_loader: DataLoader = None,
                 test_loader: DataLoader = None,
                 device: str = 'cuda',
                 learning_rate: float = 1e-4,
                 weight_decay: float = 1e-4,
                 num_epochs: int = 100,
                 save_dir: str = 'shared/checkpoints',
                 use_amp: bool = True):
        """
        åˆå§‹åŒ–PANetè®­ç»ƒå™¨
        
        Args:
            model: PANetæ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            device: è®¾å¤‡
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            num_epochs: è®­ç»ƒè½®æ•°
            save_dir: ä¿å­˜ç›®å½•
            use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.device = device
        self.num_epochs = num_epochs
        self.save_dir = save_dir
        self.use_amp = use_amp
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=num_epochs,
            eta_min=learning_rate * 0.01
        )
        
        # åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒ
        if use_amp:
            if NEW_AMP_API:
                self.scaler = GradScaler('cuda')
            else:
                self.scaler = GradScaler()
        else:
            self.scaler = None
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•° - æ”¯æŒåŠ¨æ€ç±»åˆ«æ•°
        if hasattr(model, 'num_classes'):
            num_classes = model.num_classes
        else:
            num_classes = 5
        self.criterion = FocalDiceLoss(num_classes=num_classes)
        
        # åˆå§‹åŒ–TensorBoard - ç¡®ä¿è¾“å‡ºåˆ°sharedç›®å½•
        if not save_dir.startswith('shared/'):
            save_dir = f'shared/{save_dir}'
        self.writer = SummaryWriter(os.path.join(save_dir, 'logs'))
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.best_val_dice = 0.0
        
        print(f"PANetè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ: device={device}, epochs={num_epochs}")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}')
        
        for batch_idx, batch in enumerate(pbar):
            # å‡†å¤‡æ•°æ®
            if isinstance(batch, dict):
                images = batch['image'].to(self.device)
                labels = batch['label'].to(self.device)
            else:
                images, labels = batch
                images = images.to(self.device)
                labels = labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            if self.use_amp and self.scaler:
                with autocast():
                    # MedSAMä¼˜åŒ–ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤promptç”Ÿæˆ
                    batch_size = images.shape[0]
                    points = None  # MedSAMä¼šè‡ªåŠ¨ç”Ÿæˆé»˜è®¤prompts
                    boxes = None
                    masks = None
                    
                    outputs = self.model(images, points, boxes, masks)
                    
                    # å¤„ç†å­—å…¸è¾“å‡º
                    if isinstance(outputs, dict):
                        outputs = outputs.get('cardiac_logits', outputs.get('sam_masks', outputs))
                    
                    loss = self.criterion(outputs, labels)
                
                # åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
                self.optimizer.zero_grad()
            else:
                # MedSAMä¼˜åŒ–ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤promptç”Ÿæˆ
                batch_size = images.shape[0]
                points = None  # MedSAMä¼šè‡ªåŠ¨ç”Ÿæˆé»˜è®¤prompts
                boxes = None
                masks = None
                
                outputs = self.model(images, points, boxes, masks)
                
                # MedSAMä¼˜åŒ–ï¼šå¤„ç†æ–°çš„è¾“å‡ºæ ¼å¼
                if isinstance(outputs, dict):
                    # ä¼˜å…ˆä½¿ç”¨cardiac_logitsï¼Œè¿™æ˜¯MedSAMçš„ä¸»è¦è¾“å‡º
                    outputs = outputs.get('cardiac_logits', outputs.get('predictions', outputs.get('sam_masks', outputs)))
                
                loss = self.criterion(outputs, labels)
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()
                self.optimizer.zero_grad()
            
            # æ›´æ–°ç»Ÿè®¡
            total_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{total_loss / (batch_idx + 1):.4f}'
            })
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / len(self.train_loader)
        
        return {'train_loss': avg_loss}
    
    def validate_epoch(self, epoch: int) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        if self.val_loader is None:
            return {}
        
        self.model.eval()
        total_loss = 0.0
        total_metrics = {'mean_dice': 0.0, 'pixel_accuracy': 0.0}
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validation'):
                # å‡†å¤‡æ•°æ®
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                    labels = batch['label'].to(self.device)
                else:
                    images, labels = batch
                    images = images.to(self.device)
                    labels = labels.to(self.device)
                
                # å‰å‘ä¼ æ’­
                if self.use_amp and self.scaler:
                    if NEW_AMP_API:
                        with autocast(device_type='cuda'):
                            # MedSAMä¼˜åŒ–ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤promptç”Ÿæˆ
                            batch_size = images.shape[0]
                            points = None  # MedSAMä¼šè‡ªåŠ¨ç”Ÿæˆé»˜è®¤prompts
                            boxes = None
                            masks = None
                            
                            outputs = self.model(images, points, boxes, masks)
                            
                            # å¤„ç†å­—å…¸è¾“å‡º
                            if isinstance(outputs, dict):
                                outputs = outputs.get('cardiac_logits', outputs.get('sam_masks', outputs))
                            
                            loss = self.criterion(outputs, labels)
                    else:
                        with autocast():
                            # MedSAMä¼˜åŒ–ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤promptç”Ÿæˆ
                            batch_size = images.shape[0]
                            points = None  # MedSAMä¼šè‡ªåŠ¨ç”Ÿæˆé»˜è®¤prompts
                            boxes = None
                            masks = None
                            
                            outputs = self.model(images, points, boxes, masks)
                            
                            # å¤„ç†å­—å…¸è¾“å‡º
                            if isinstance(outputs, dict):
                                outputs = outputs.get('cardiac_logits', outputs.get('sam_masks', outputs))
                            
                            loss = self.criterion(outputs, labels)
                else:
                    # MedSAMä¼˜åŒ–ï¼šä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤promptç”Ÿæˆ
                    batch_size = images.shape[0]
                    points = None  # MedSAMä¼šè‡ªåŠ¨ç”Ÿæˆé»˜è®¤prompts
                    boxes = None
                    masks = None
                    
                    outputs = self.model(images, points, boxes, masks)
                    
                    # å¤„ç†å­—å…¸è¾“å‡º
                    if isinstance(outputs, dict):
                        outputs = outputs.get('cardiac_logits', outputs.get('sam_masks', outputs))
                    
                    loss = self.criterion(outputs, labels)
                
                # æ›´æ–°ç»Ÿè®¡
                total_loss += loss.item()
                
                # è®¡ç®—æŒ‡æ ‡ - ä½¿ç”¨ä¸æ¨ç†ä¸€è‡´çš„argmaxæ–¹æ³•
                pred_classes = torch.argmax(outputs, dim=1)  # ç›´æ¥ä½¿ç”¨argmax
                metrics = TrainingUtils.calculate_metrics(pred_classes.unsqueeze(1), labels)
                for key, value in metrics.items():
                    if key in total_metrics:
                        total_metrics[key] += value
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = total_loss / len(self.val_loader)
        avg_metrics = {key: value / len(self.val_loader) for key, value in total_metrics.items()}
        
        return {'val_loss': avg_loss, **avg_metrics}
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("å¼€å§‹PANetæ¨¡å‹è®­ç»ƒ...")
        
        # è®°å½•è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        val_dices = []
        
        for epoch in range(self.num_epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(epoch)
            
            # éªŒè¯
            val_metrics = self.validate_epoch(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # è®°å½•æŒ‡æ ‡
            all_metrics = {**train_metrics, **val_metrics}
            for key, value in all_metrics.items():
                self.writer.add_scalar(key, value, epoch)
            
            # è®°å½•å†å²
            train_losses.append(train_metrics.get('train_loss', 0))
            val_losses.append(val_metrics.get('val_loss', 0))
            val_dices.append(val_metrics.get('mean_dice', 0))
            
            # æ‰“å°è¿›åº¦
            print(f"Epoch {epoch+1}/{self.num_epochs}: "
                  f"Train Loss: {train_metrics.get('train_loss', 0):.4f}, "
                  f"Val Loss: {val_metrics.get('val_loss', 0):.4f}, "
                  f"Val Dice: {val_metrics.get('mean_dice', 0):.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics.get('val_loss', float('inf')) < self.best_val_loss:
                self.best_val_loss = val_metrics.get('val_loss', float('inf'))
                self.save_checkpoint(is_best=True)
            
            if val_metrics.get('mean_dice', 0) > self.best_val_dice:
                self.best_val_dice = val_metrics.get('mean_dice', 0)
                self.save_checkpoint(is_best=True, suffix='_best_dice')
        
        print("PANetæ¨¡å‹è®­ç»ƒå®Œæˆ!")
        self.writer.close()
        
        # è¿”å›è®­ç»ƒç»“æœ
        return {
            'final_train_loss': train_losses[-1] if train_losses else 0,
            'final_val_loss': val_losses[-1] if val_losses else 0,
            'final_val_dice': val_dices[-1] if val_dices else 0,
            'best_val_loss': self.best_val_loss,
            'best_val_dice': self.best_val_dice,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'val_dices': val_dices
        }
    
    def save_checkpoint(self, is_best: bool = False, suffix: str = ''):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = os.path.join(
            self.save_dir, 
            f'panet_checkpoint_epoch_{self.current_epoch}{suffix}.pth'
        )
        
        TrainingUtils.save_checkpoint(
            model=self.model,
            optimizer=self.optimizer,
            scheduler=self.scheduler,
            epoch=self.current_epoch,
            loss=self.best_val_loss,
            save_path=checkpoint_path,
            best_val_dice=self.best_val_dice
        )

# ============================================================================
# å·¥å‚å‡½æ•°
# ============================================================================

def create_sam_trainer(model: nn.Module,
                      train_loader: DataLoader,
                      val_loader: DataLoader = None,
                      test_loader: DataLoader = None,
                      device: str = 'cuda',
                      learning_rate: float = 1e-4,
                      weight_decay: float = 1e-4,
                      num_epochs: int = 100,
                      save_dir: str = 'shared/checkpoints',
                      use_amp: bool = True,
                      use_checkpoint: bool = False,
                      accumulation_steps: int = 1,
                      early_stopping_patience: int = 5,
                      criterion: nn.Module = None) -> SAMTrainer:
    """
    åˆ›å»ºSAMè®­ç»ƒå™¨
    
    Args:
        model: SAMæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        learning_rate: å­¦ä¹ ç‡
        weight_decay: æƒé‡è¡°å‡
        num_epochs: è®­ç»ƒè½®æ•°
        save_dir: ä¿å­˜ç›®å½•
        use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        use_checkpoint: æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        early_stopping_patience: æ—©åœè€å¿ƒå€¼ï¼ˆè¿ç»­å¤šå°‘è½®æ— æ”¹å–„ååœæ­¢ï¼‰
    
    Returns:
        SAMTrainerå¯¹è±¡
    """
    return SAMTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        test_loader=test_loader,
        device=device,
        learning_rate=learning_rate,
        weight_decay=weight_decay,
        num_epochs=num_epochs,
        save_dir=save_dir,
        use_amp=use_amp,
        use_checkpoint=use_checkpoint,
        accumulation_steps=accumulation_steps,
        early_stopping_patience=early_stopping_patience,
        criterion=criterion
    )

def create_panet_trainer(model: nn.Module,
                        train_loader: DataLoader,
                        val_loader: DataLoader = None,
                        test_loader: DataLoader = None,
                        device: str = 'cuda',
                        learning_rate: float = 1e-4,
                        weight_decay: float = 1e-4,
                        num_epochs: int = 100,
                        save_dir: str = 'shared/checkpoints',
                        use_amp: bool = True) -> PANetTrainer:
    """
    åˆ›å»ºPANetè®­ç»ƒå™¨
    
    Args:
        model: PANetæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        learning_rate: å­¦ä¹ ç‡
        weight_decay: æƒé‡è¡°å‡
        num_epochs: è®­ç»ƒè½®æ•°
        save_dir: ä¿å­˜ç›®å½•
        use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    
    Returns:
        PANetTrainerå¯¹è±¡
    """
    return PANetTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        test_loader=test_loader,
        device=device,
        learning_rate=learning_rate,
        weight_decay=weight_decay,
        num_epochs=num_epochs,
        save_dir=save_dir,
        use_amp=use_amp
    )
