#!/usr/bin/env python3
"""
å¿ƒè„å››è…”åˆ†å‰²SAMæ¨¡å‹
åŸºäºSAMæ¶æ„ï¼Œä¸“é—¨ç”¨äºå¿ƒè„è¶…å£°å›¾åƒçš„å››è…”åˆ†å‰²
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional, Tuple
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.sam.sam_model import SAMModel
from core.panet.panet_model import PANetModule


class GradientSafeUpsample(nn.Upsample):
    """æ¢¯åº¦å®‰å…¨çš„ä¸Šé‡‡æ ·å±‚ - ç¡®ä¿æ¢¯åº¦èƒ½å¤Ÿæ­£ç¡®å›æµ"""
    
    def forward(self, input):
        output = super().forward(input)
        # ç¡®ä¿æ¢¯åº¦èƒ½å¤Ÿå›æµ
        if input.requires_grad and not output.requires_grad:
            output.requires_grad_(True)
        return output


class ImprovedFeaturePyramidUpsampler(nn.Module):
    """æ”¹è¿›çš„ç‰¹å¾é‡‘å­—å¡”ä¸Šé‡‡æ ·å™¨ - æ”¯æŒå¤šå°ºåº¦è¾“å…¥å¹¶ç¡®ä¿æ¢¯åº¦æµåŠ¨"""
    
    def __init__(self, in_channels=256, out_channels=256):
        super().__init__()
        
        # 32Ã—32 â†’ 512Ã—512 çš„æ ‡å‡†ä¸Šé‡‡æ ·è·¯å¾„
        self.upsample_32_to_512 = nn.Sequential(
            # 32â†’64
            nn.Conv2d(in_channels, 192, 3, padding=1),
            nn.BatchNorm2d(192),
            nn.ReLU(),
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False),
            
            # 64â†’128  
            nn.Conv2d(192, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False),
            
            # 128â†’256
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False),
            
            # 256â†’512
            nn.Conv2d(64, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False)
        )
        
        # 16Ã—16 â†’ 512Ã—512 çš„ä¸“é—¨è·¯å¾„
        self.upsample_16_to_512 = nn.Sequential(
            # 16â†’32ï¼ˆå…ˆ2å€ä¸Šé‡‡æ ·ï¼‰
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False),
            nn.Conv2d(in_channels, 192, 3, padding=1),
            nn.BatchNorm2d(192),
            nn.ReLU(),
            
            # 32â†’64
            nn.Conv2d(192, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False),
            
            # 64â†’128  
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False),
            
            # 128â†’256
            nn.Conv2d(64, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False),
            
            # 256â†’512
            nn.Conv2d(32, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False)
        )
        
        # 64Ã—64 â†’ 512Ã—512 çš„è·¯å¾„
        self.upsample_64_to_512 = nn.Sequential(
            # 64â†’128
            nn.Conv2d(in_channels, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False),
            
            # 128â†’256
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False),
            
            # 256â†’512
            nn.Conv2d(64, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            GradientSafeUpsample(scale_factor=2, mode='bilinear', align_corners=False)
        )
    
    def forward(self, x):
        _, _, h, w = x.shape
        
        if h == 16 and w == 16:
            return self.upsample_16_to_512(x)
        elif h == 32 and w == 32:
            return self.upsample_32_to_512(x)
        elif h == 64 and w == 64:
            return self.upsample_64_to_512(x)
        else:
            # å¯¹äºå…¶ä»–å°ºå¯¸ï¼Œä½¿ç”¨è‡ªé€‚åº”ä¸Šé‡‡æ ·
            return self._adaptive_upsample(x)
    
    def _adaptive_upsample(self, x):
        """è‡ªé€‚åº”ä¸Šé‡‡æ ·åˆ°512Ã—512"""
        target_size = (512, 512)
        
        # ç›´æ¥ä¸Šé‡‡æ ·åˆ°ç›®æ ‡å°ºå¯¸
        upsampled = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)
        
        # é€šè¿‡ä¸€ä¸ªå·ç§¯å±‚å¤„ç†
        conv_layer = nn.Conv2d(x.shape[1], x.shape[1], 3, padding=1).to(x.device)
        output = conv_layer(upsampled)
        
        return output
    
    def check_gradient_flow(self, input_tensor):
        """æ£€æŸ¥æ¢¯åº¦æµåŠ¨çš„è¾…åŠ©å‡½æ•°"""
        print(f"ğŸ” æ¢¯åº¦æµåŠ¨æ£€æŸ¥:")
        print(f"è¾“å…¥å¼ é‡éœ€è¦æ¢¯åº¦: {input_tensor.requires_grad}")
        print(f"è¾“å…¥å¼ é‡æ˜¯å¶å­èŠ‚ç‚¹: {input_tensor.is_leaf}")
        
        # ç¡®ä¿è¾“å…¥å¼ é‡ä¿ç•™æ¢¯åº¦
        if not input_tensor.is_leaf:
            input_tensor.retain_grad()
        
        output = self.forward(input_tensor)
        print(f"è¾“å‡ºå¼ é‡éœ€è¦æ¢¯åº¦: {output.requires_grad}")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # åˆ›å»ºæŸå¤±å¹¶åå‘ä¼ æ’­
        loss = output.mean()
        print(f"æŸå¤±å€¼: {loss.item():.6f}")
        
        loss.backward()
        
        # æ£€æŸ¥è¾“å…¥æ¢¯åº¦
        if input_tensor.grad is not None:
            input_grad_norm = input_tensor.grad.norm().item()
            # é™é»˜è®°å½•è¾“å…¥æ¢¯åº¦èŒƒæ•°
            input_has_grad = True
        else:
            print(f"âŒ è¾“å…¥æ¢¯åº¦ä¸ºNone")
            input_has_grad = False
        
        # åªæ£€æŸ¥å®é™…ä½¿ç”¨çš„è·¯å¾„çš„å‚æ•°æ¢¯åº¦
        _, _, h, w = input_tensor.shape
        
        if h == 16 and w == 16:
            used_path = "upsample_16_to_512"
        elif h == 32 and w == 32:
            used_path = "upsample_32_to_512"
        elif h == 64 and w == 64:
            used_path = "upsample_64_to_512"
        else:
            used_path = "adaptive"
        
        print(f"ä½¿ç”¨çš„è·¯å¾„: {used_path}")
        
        # æ£€æŸ¥ä½¿ç”¨è·¯å¾„çš„å‚æ•°æ¢¯åº¦
        used_param_count = 0
        used_grad_count = 0
        
        for name, param in self.named_parameters():
            if used_path in name or used_path == "adaptive":
                used_param_count += 1
                if param.grad is not None:
                    used_grad_count += 1
                    # é™é»˜è®°å½•å‚æ•°æ¢¯åº¦èŒƒæ•°
                else:
                    print(f"âŒ {name}: æ— æ¢¯åº¦")
        
        print(f"ä½¿ç”¨è·¯å¾„å‚æ•°æ¢¯åº¦ç»Ÿè®¡: {used_grad_count}/{used_param_count}")
        
        # å¦‚æœæ˜¯è‡ªé€‚åº”è·¯å¾„ï¼Œæ£€æŸ¥æ‰€æœ‰å‚æ•°
        if used_path == "adaptive":
            total_param_count = 0
            total_grad_count = 0
            for name, param in self.named_parameters():
                total_param_count += 1
                if param.grad is not None:
                    total_grad_count += 1
            print(f"æ€»å‚æ•°æ¢¯åº¦ç»Ÿè®¡: {total_grad_count}/{total_param_count}")
            return input_has_grad and (total_grad_count > 0)
        
        return input_has_grad and (used_grad_count == used_param_count)


# ä¿æŒå‘åå…¼å®¹æ€§çš„åˆ«å
FeaturePyramidUpsampler = ImprovedFeaturePyramidUpsampler


class CardiacSAM(nn.Module):
    """
    å¿ƒè„å››è…”åˆ†å‰²SAMæ¨¡å‹
    
    ç»“åˆSAMçš„å›¾åƒç¼–ç èƒ½åŠ›å’ŒPANetçš„ç‰¹å¾èåˆèƒ½åŠ›ï¼Œ
    ä¸“é—¨ç”¨äºå¿ƒè„è¶…å£°å›¾åƒçš„å››è…”åˆ†å‰²ä»»åŠ¡
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__()
        
        if config is None:
            from configs.cardiac_config import CardiacConfig
            config = CardiacConfig().get_model_config('cardiac_sam')
        
        self.config = config
        self.num_classes = config.get('num_classes', 5)
        self.feature_dim = config.get('feature_dim', 256)
        self.use_hq = config.get('use_hq', False)
        self.use_sam_features = config.get('use_sam_features', True)
        self.target_size = config.get('image_size', (512, 512))
        
        # SAMæ¨¡å‹ä½œä¸ºä¸»å¹²ç½‘ç»œ
        self.sam_model = SAMModel(config)
        
        # è½»é‡åŒ–åŒ»å­¦å›¾åƒé€‚é…å±‚ - å‡å°‘æ˜¾å­˜æ¶ˆè€—
        self.medical_adaptation = nn.Sequential(
            nn.Conv2d(3, 3, 1),  # ç®€åŒ–ä¸º1x1å·ç§¯ï¼Œå‡å°‘è®¡ç®—é‡
            nn.Tanh()  # é™åˆ¶è¾“å‡ºèŒƒå›´
        )
        
        # åŠ è½½SAMé¢„è®­ç»ƒæƒé‡
        sam_checkpoint_path = config.get('sam_checkpoint_path')
        if sam_checkpoint_path and os.path.exists(sam_checkpoint_path):
            self._load_sam_weights(sam_checkpoint_path)
        else:
            pass  # é™é»˜åˆå§‹åŒ–
        
        # PANetç‰¹å¾èåˆæ¨¡å—
        self.panet_fusion = PANetModule()
        
        # ç‰¹å¾å¢å¼ºæ¨¡å— - æé«˜ç©ºé—´å˜åŒ–æ€§
        self.feature_enhancer = nn.Sequential(
            # ç©ºé—´æ³¨æ„åŠ›æ¨¡å—
            nn.Conv2d(self.feature_dim, self.feature_dim // 4, 1),
            nn.BatchNorm2d(self.feature_dim // 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(self.feature_dim // 4, self.feature_dim, 1),
            nn.Sigmoid()
        )
        
        # æ”¹è¿›çš„å¿ƒè„åˆ†å‰²å¤´ - æ”¯æŒåŠ¨æ€ç±»åˆ«æ•°è°ƒæ•´
        self.cardiac_head = nn.Sequential(
            # ç¬¬ä¸€ç»„ï¼šç‰¹å¾æå–
            nn.Conv2d(self.feature_dim, 128, 3, padding=1, groups=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.1),
            
            # ç¬¬äºŒç»„ï¼šç©ºé—´ç»†åŒ–
            nn.Conv2d(128, 128, 3, padding=2, dilation=2),  # è†¨èƒ€å·ç§¯ï¼Œè°ƒæ•´paddingä¿æŒå°ºå¯¸
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            # ç¬¬ä¸‰ç»„ï¼šé€šé“å‹ç¼©
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.05),
            
            # æœ€ç»ˆåˆ†ç±»å±‚ - åˆå§‹åŒ–ä¸ºæœ€å¤§ç±»åˆ«æ•°ï¼Œåç»­åŠ¨æ€è°ƒæ•´
            nn.Conv2d(64, 5, 1)  # åˆå§‹åŒ–ä¸º5ç±»ï¼Œè®­ç»ƒæ—¶ä¼šåŠ¨æ€è°ƒæ•´
        )
        
        # æ”¹è¿›çš„åˆå§‹åŒ– - ä½¿ç”¨æ›´å¥½çš„æƒé‡åˆå§‹åŒ–
        self._init_cardiac_head()
        
        # ç‰¹å¾æŠ•å½±å±‚ï¼ˆå¦‚æœéœ€è¦è°ƒæ•´ç‰¹å¾ç»´åº¦ï¼‰
        if self.feature_dim != 256:
            self.feature_proj = nn.Conv2d(256, self.feature_dim, 1)
        else:
            self.feature_proj = nn.Identity()
    
    def _init_cardiac_head(self):
        """åˆå§‹åŒ–å¿ƒè„åˆ†å‰²å¤´çš„æƒé‡"""
        for m in self.cardiac_head.modules():
            if isinstance(m, nn.Conv2d):
                # ä½¿ç”¨Heåˆå§‹åŒ–ï¼Œå¢åŠ æƒé‡æ–¹å·®
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    # ä¸ºæœ€åä¸€å±‚æ·»åŠ ç±»åˆ«åç½®
                    if m.out_channels == self.num_classes:
                        # ç»™ä¸åŒç±»åˆ«è®¾ç½®æ›´å¹³è¡¡çš„åˆå§‹åç½®
                        with torch.no_grad():
                            for i in range(self.num_classes):
                                m.bias[i] = 0.0  # æ‰€æœ‰ç±»åˆ«éƒ½è®¾ä¸ºé›¶åç½®
                    else:
                        nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, images: torch.Tensor,
                points: Optional[torch.Tensor] = None,
                boxes: Optional[torch.Tensor] = None,
                masks: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            images: è¾“å…¥å›¾åƒ [B, C, H, W]
            points: ç‚¹æç¤º [B, N, 3] (å¯é€‰)
            boxes: æ¡†æç¤º [B, N, 4] (å¯é€‰)
            masks: æ©ç æç¤º [B, N, H, W] (å¯é€‰)
        
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        batch_size = images.shape[0]
        
        # åŒ»å­¦å›¾åƒé€‚é…é¢„å¤„ç† - æ£€æŸ¥è¾“å…¥æ ¼å¼
        # å¤„ç†3DåŒ»å­¦å›¾åƒ
        if len(images.shape) == 5:  # (B, C, D, H, W)
            # å–ä¸­é—´åˆ‡ç‰‡è¿›è¡Œ2Då¤„ç†
            mid_slice = images.shape[2] // 2
            images = images[:, :, mid_slice, :, :]
        
        # è®°å½•åŸå§‹å›¾åƒå°ºå¯¸ï¼Œç”¨äºåç»­è¾“å‡ºåŒ¹é…
        original_size = images.shape[-2:]
        
        # ç¡®ä¿å›¾åƒå°ºå¯¸æ­£ç¡®ï¼ˆæ ¹æ®é…ç½®åŠ¨æ€è°ƒæ•´ï¼‰
        target_size = getattr(self, 'target_size', (512, 512))
        if images.shape[-2:] != target_size:
            images = torch.nn.functional.interpolate(
                images, size=target_size, mode='bilinear', align_corners=False
            )
        
        if images.dim() == 4 and images.shape[1] != 3:
            # å¦‚æœè¾“å…¥ä¸æ˜¯æ ‡å‡†çš„[B, 3, H, W]æ ¼å¼ï¼Œéœ€è¦è°ƒæ•´
            if images.shape[1] == 1:
                # å•é€šé“è½¬ä¸‰é€šé“
                images = images.repeat(1, 3, 1, 1)
            elif images.shape[1] > 3:
                # å¤šé€šé“å–å‰ä¸‰ä¸ª
                images = images[:, :3, :, :]
        
        adapted_images = self.medical_adaptation(images)
        
        # å¦‚æœæ²¡æœ‰æä¾›æç¤ºï¼Œåˆ›å»ºé»˜è®¤æç¤º
        if points is None and boxes is None and masks is None:
            # åˆ›å»ºå›¾åƒä¸­å¿ƒç‚¹ä½œä¸ºé»˜è®¤æç¤º - é€‚é…æ–°çš„MedSAMæ ¼å¼
            h, w = adapted_images.shape[-2:]
            center_point = torch.tensor([[w//2, h//2]], 
                                     device=adapted_images.device, 
                                     dtype=torch.float32)
            center_label = torch.tensor([[1]], 
                                      device=adapted_images.device, 
                                      dtype=torch.int32)
            points = (center_point.expand(batch_size, -1, -1),
                     center_label.expand(batch_size, -1))
        
        # SAMæ¨¡å‹å‰å‘ä¼ æ’­ - ä½¿ç”¨é€‚é…åçš„å›¾åƒ
        sam_outputs = self.sam_model(adapted_images, points, boxes, masks)
        
        # ä½¿ç”¨SAMçš„å®Œæ•´è¾“å‡ºï¼ŒåŒ…æ‹¬promptä¿¡æ¯
        if 'sam_features' in sam_outputs:
            sam_features = sam_outputs['sam_features']
        else:
            # å¦‚æœæ²¡æœ‰sam_featuresï¼Œåˆ™ä½¿ç”¨image_encoder
            with torch.no_grad():  # å†»ç»“SAMä¸»å¹²ç½‘ç»œ
                sam_features = self.sam_model.image_encoder(adapted_images)
        
        # è°ƒæ•´ç‰¹å¾ç»´åº¦
        sam_features = self.feature_proj(sam_features)
        
        # åˆ›å»ºå¤šå°ºåº¦ç‰¹å¾ç”¨äºPANetèåˆ
        if self.use_sam_features:
            # åˆ›å»ºä¸åŒå°ºåº¦çš„ç‰¹å¾å›¾
            multi_scale_features = []
            
            # åŸå§‹ç‰¹å¾ï¼ˆæœ€é«˜åˆ†è¾¨ç‡ï¼‰
            if sam_features.shape[-2:] != images.shape[-2:]:
                # ä¸Šé‡‡æ ·åˆ°è¾“å…¥å›¾åƒå°ºå¯¸
                upsampled_features = F.interpolate(
                    sam_features,
                    size=images.shape[-2:],
                    mode='bilinear',
                    align_corners=False
                )
                multi_scale_features.append(upsampled_features)
            else:
                multi_scale_features.append(sam_features)
            
            # å‡å°‘å¤šå°ºåº¦ç‰¹å¾ä»¥æé«˜é€Ÿåº¦
            scales = [0.5, 0.25]  # åªç”¨ä¸¤ä¸ªå°ºåº¦ï¼Œå‡å°‘è®¡ç®—é‡
            for scale in scales:
                target_size = (int(images.shape[-2] * scale), int(images.shape[-1] * scale))
                if target_size[0] > 16 and target_size[1] > 16:  # æé«˜æœ€å°å°ºå¯¸é˜ˆå€¼
                    scaled_features = F.interpolate(
                        sam_features,
                        size=target_size,
                        mode='bilinear',
                        align_corners=False
                    )
                    # å†ä¸Šé‡‡æ ·å›åŸå§‹å°ºå¯¸
                    scaled_features = F.interpolate(
                        scaled_features,
                        size=images.shape[-2:],
                        mode='bilinear',
                        align_corners=False
                    )
                    multi_scale_features.append(scaled_features)
            
            # ä½¿ç”¨PANetèåˆå¤šå°ºåº¦ç‰¹å¾
            fused_features = self.panet_fusion(multi_scale_features)
            
            # å…³é”®ä¿®å¤ï¼šåº”ç”¨ç‰¹å¾å¢å¼ºï¼Œæé«˜ç©ºé—´å˜åŒ–æ€§
            attention_weights = self.feature_enhancer(fused_features)
            enhanced_features = fused_features * attention_weights
        else:
            # ç›´æ¥ä½¿ç”¨SAMç‰¹å¾
            if sam_features.shape[-2:] != images.shape[-2:]:
                fused_features = F.interpolate(
                    sam_features,
                    size=images.shape[-2:],
                    mode='bilinear',
                    align_corners=False
                )
            else:
                fused_features = sam_features
            
            # å¯¹ç›´æ¥SAMç‰¹å¾ä¹Ÿåº”ç”¨å¢å¼º
            attention_weights = self.feature_enhancer(fused_features)
            enhanced_features = fused_features * attention_weights
        
        # å¿ƒè„åˆ†å‰²é¢„æµ‹ - ä½¿ç”¨å¢å¼ºåçš„ç‰¹å¾
        cardiac_logits = self.cardiac_head(enhanced_features)
        
        # ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸åŸå§‹è¾“å…¥å°ºå¯¸åŒ¹é…
        if cardiac_logits.shape[-2:] != original_size:
            cardiac_logits = F.interpolate(
                cardiac_logits, size=original_size, mode='bilinear', align_corners=False
            )
        
        # ç¡®ä¿è¾“å‡ºå¼ é‡å½¢çŠ¶æ­£ç¡®ï¼š[B, C, H, W]
        if cardiac_logits.dim() != 4:
            raise ValueError(f"cardiac_logits ç»´åº¦é”™è¯¯: {cardiac_logits.shape}, æœŸæœ›: [B, C, H, W]")
        
        # åŠ¨æ€æ›´æ–°æ¨¡å‹ç±»åˆ«æ•°ï¼ˆç”¨äºä¸‰é˜¶æ®µè®­ç»ƒï¼‰
        if cardiac_logits.shape[1] != self.num_classes:
            self.num_classes = cardiac_logits.shape[1]
        
        # ç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡® - æ”¯æŒ2ã€3ã€5ç±»
        if cardiac_logits.shape[1] not in [2, 3, 5]:
            raise ValueError(f"è¾“å‡ºç±»åˆ«æ•°é”™è¯¯: {cardiac_logits.shape[1]}, æœŸæœ›: 2ã€3 æˆ– 5")
        
        # æ„å»ºè¾“å‡º
        outputs = {
            'cardiac_logits': cardiac_logits,
            'sam_masks': sam_outputs.get('masks', None),
            'iou_pred': sam_outputs.get('iou_pred', None),
            'sam_features': sam_features,
            'fused_features': enhanced_features
        }
        
        # ä¸ºäº†å…¼å®¹è®­ç»ƒè„šæœ¬ï¼Œæ·»åŠ æ ‡å‡†é¢„æµ‹è¾“å‡º
        outputs['predictions'] = cardiac_logits
        
        return outputs
    
    def get_sam_features(self, images: torch.Tensor) -> torch.Tensor:
        """è·å–SAMç‰¹å¾ï¼ˆç”¨äºç‰¹å¾åˆ†æï¼‰"""
        with torch.no_grad():
            sam_outputs = self.sam_model(images)
            if 'image_embeddings' in sam_outputs:
                return sam_outputs['image_embeddings']
            else:
                return self.sam_model.image_encoder(
                    self.sam_model.preprocess_images(images)
                )
    
    def predict_cardiac_segmentation(self, images: torch.Tensor, 
                                   use_temperature_scaling: bool = True,
                                   temperature: float = 2.0) -> torch.Tensor:
        """
        é¢„æµ‹å¿ƒè„åˆ†å‰²ï¼ˆæ¨ç†æ¥å£ï¼‰
        
        Args:
            images: è¾“å…¥å›¾åƒ
            use_temperature_scaling: æ˜¯å¦ä½¿ç”¨æ¸©åº¦ç¼©æ”¾
            temperature: æ¸©åº¦å‚æ•°ï¼Œ>1ä¼šä½¿é¢„æµ‹æ›´å¹³æ»‘
        
        Returns:
            é¢„æµ‹çš„logitsæˆ–æ¦‚ç‡åˆ†å¸ƒ
        """
        with torch.no_grad():
            outputs = self.forward(images)
            logits = outputs['cardiac_logits']
            
            if use_temperature_scaling:
                # ä½¿ç”¨æ¸©åº¦ç¼©æ”¾æ¥å¹³è¡¡é¢„æµ‹æ¦‚ç‡
                logits = logits / temperature
                
            return logits
    
    def predict_with_postprocessing(self, images: torch.Tensor) -> torch.Tensor:
        """
        å¸¦åå¤„ç†çš„é¢„æµ‹ï¼ˆè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼‰
        """
        with torch.no_grad():
            outputs = self.forward(images)
            logits = outputs['cardiac_logits']
            
            # æ–¹æ³•1: æ·»åŠ éšæœºå™ªå£°å¢åŠ ç©ºé—´å˜åŒ–æ€§
            if self.training:
                noise = torch.randn_like(logits) * 0.1
                logits = logits + noise
            
            # æ–¹æ³•2: ä½¿ç”¨Gumbel-Softmaxå¢åŠ éšæœºæ€§
            temperature = 1.0
            gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-8) + 1e-8)
            gumbel_logits = (logits + gumbel_noise) / temperature
            
            # æ–¹æ³•3: ç±»åˆ«å¹³è¡¡çš„é˜ˆå€¼è°ƒæ•´
            class_thresholds = torch.tensor([-0.2, 0.1, -0.1, 0.05, 0.0], 
                                          device=logits.device).view(1, -1, 1, 1)
            adjusted_logits = gumbel_logits + class_thresholds
            
            return adjusted_logits
    
    def predict_with_spatial_diversity(self, images: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨ç©ºé—´å¤šæ ·æ€§çš„é¢„æµ‹æ–¹æ³•
        """
        with torch.no_grad():
            outputs = self.forward(images)
            logits = outputs['cardiac_logits']
            
            B, C, H, W = logits.shape
            
            # åˆ›å»ºç©ºé—´å˜åŒ–çš„æƒé‡
            y_coords = torch.linspace(-1, 1, H, device=logits.device).view(1, 1, H, 1)
            x_coords = torch.linspace(-1, 1, W, device=logits.device).view(1, 1, 1, W)
            
            # ä¸ºä¸åŒç±»åˆ«åˆ›å»ºä¸åŒçš„ç©ºé—´åå¥½
            spatial_weights = torch.zeros_like(logits)
            
            # èƒŒæ™¯ï¼šè¾¹ç¼˜åŒºåŸŸæ›´å¯èƒ½
            spatial_weights[:, 0] = (y_coords.abs() + x_coords.abs()) * 0.5
            
            # å·¦å¿ƒå®¤ï¼šå·¦ä¸‹åŒºåŸŸæ›´å¯èƒ½  
            spatial_weights[:, 1] = -(x_coords + 0.3) * (y_coords - 0.3) * 2.0
            
            # å³å¿ƒå®¤ï¼šå³ä¸‹åŒºåŸŸæ›´å¯èƒ½
            spatial_weights[:, 2] = (x_coords - 0.3) * (y_coords - 0.3) * 2.0
            
            # å·¦å¿ƒæˆ¿ï¼šå·¦ä¸ŠåŒºåŸŸæ›´å¯èƒ½
            spatial_weights[:, 3] = -(x_coords + 0.3) * (y_coords + 0.3) * 2.0
            
            # å³å¿ƒæˆ¿ï¼šå³ä¸ŠåŒºåŸŸæ›´å¯èƒ½
            spatial_weights[:, 4] = (x_coords - 0.3) * (y_coords + 0.3) * 2.0
            
            # åº”ç”¨ç©ºé—´æƒé‡
            enhanced_logits = logits + spatial_weights * 0.3
            
            return enhanced_logits
    
    def predict_cardiac_with_diversity(self, images: torch.Tensor, 
                                     method: str = "spatial_diversity") -> Dict[str, torch.Tensor]:
        """
        å¤šæ ·æ€§å¿ƒè„åˆ†å‰²é¢„æµ‹ - ä¸»è¦æ¨ç†æ¥å£
        
        Args:
            images: è¾“å…¥å›¾åƒ [B, C, H, W]
            method: é¢„æµ‹æ–¹æ³• ("spatial_diversity", "postprocessing", "temperature")
        
        Returns:
            åŒ…å«å¤šç§é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        with torch.no_grad():
            # åŸå§‹é¢„æµ‹
            original_outputs = self.forward(images)
            original_logits = original_outputs['cardiac_logits']
            original_pred = torch.argmax(original_logits, dim=1)
            
            # åº”ç”¨é€‰æ‹©çš„å¤šæ ·æ€§æ–¹æ³•
            if method == "spatial_diversity":
                enhanced_logits = self.predict_with_spatial_diversity(images)
            elif method == "postprocessing":
                enhanced_logits = self.predict_with_postprocessing(images)
            elif method == "temperature":
                enhanced_logits = self.predict_cardiac_segmentation(images, 
                                                                  use_temperature_scaling=True, 
                                                                  temperature=2.0)
            else:
                enhanced_logits = original_logits
            
            enhanced_pred = torch.argmax(enhanced_logits, dim=1)
            enhanced_probs = torch.softmax(enhanced_logits, dim=1)
            
            return {
                'original_logits': original_logits,
                'enhanced_logits': enhanced_logits,
                'original_predictions': original_pred,
                'enhanced_predictions': enhanced_pred,
                'enhanced_probabilities': enhanced_probs,
                'cardiac_logits': enhanced_logits,  # ä¸ºäº†å…¼å®¹æ€§
                'predictions': enhanced_pred        # ä¸ºäº†å…¼å®¹æ€§
            }
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'num_classes': self.num_classes,
            'feature_dim': self.feature_dim,
            'use_hq': self.use_hq,
            'use_sam_features': self.use_sam_features
        }
    
    def _load_sam_weights(self, checkpoint_path: str):
        """åŠ è½½SAMé¢„è®­ç»ƒæƒé‡"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            # åªåŠ è½½image_encoderçš„æƒé‡ï¼ˆå†»ç»“ä¸»å¹²ï¼‰
            sam_image_encoder_keys = self.sam_model.image_encoder.state_dict().keys()
            filtered_state_dict = {}
            
            for key, value in checkpoint.items():
                if key.startswith('image_encoder.'):
                    # ç§»é™¤image_encoderå‰ç¼€
                    new_key = key.replace('image_encoder.', '')
                    if new_key in sam_image_encoder_keys:
                        # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
                        expected_shape = self.sam_model.image_encoder.state_dict()[new_key].shape
                        if value.shape == expected_shape:
                            filtered_state_dict[new_key] = value
                        elif new_key == 'pos_embed' and len(value.shape) == 4 and len(expected_shape) == 4:
                            # ç‰¹æ®Šå¤„ç†ä½ç½®ç¼–ç ï¼šä» (1, 64, 64, 768) è½¬æ¢ä¸º (1, 768, 64, 64)
                            if value.shape == (1, 64, 64, 768) and expected_shape == (1, 768, 64, 64):
                                filtered_state_dict[new_key] = value.permute(0, 3, 1, 2)
                                # é™é»˜è½¬æ¢ä½ç½®ç¼–ç å½¢çŠ¶
                            else:
                                # é™é»˜è·³è¿‡å½¢çŠ¶ä¸åŒ¹é…çš„æƒé‡
                                pass
                        elif 'rel_pos' in new_key and len(value.shape) == 2 and len(expected_shape) == 2:
                            # ç‰¹æ®Šå¤„ç†ç›¸å¯¹ä½ç½®ç¼–ç ï¼šæ’å€¼åˆå§‹åŒ–
                            if value.shape[1] == expected_shape[1]:  # é€šé“æ•°åŒ¹é…
                                filtered_state_dict[new_key] = self._interpolate_rel_pos(value, expected_shape)
                                # é™é»˜æ’å€¼ç›¸å¯¹ä½ç½®ç¼–ç 
                            else:
                                # é™é»˜è·³è¿‡å½¢çŠ¶ä¸åŒ¹é…çš„æƒé‡
                                pass
                        else:
                            # é™é»˜è·³è¿‡å½¢çŠ¶ä¸åŒ¹é…çš„æƒé‡
                            pass
                    else:
                        # é™é»˜è·³è¿‡ä¸åœ¨image_encoderä¸­çš„æƒé‡
                        pass
            
            # éä¸¥æ ¼åŠ è½½image_encoderæƒé‡ï¼ˆå…è®¸éƒ¨åˆ†æƒé‡ä¸åŒ¹é…ï¼‰
            missing_keys, unexpected_keys = self.sam_model.image_encoder.load_state_dict(
                filtered_state_dict, strict=False
            )

            # é™é»˜åŠ è½½æƒé‡
            
            # å…³é”®ä¿®å¤ï¼šéƒ¨åˆ†è§£å†»SAMä¸»å¹²ç½‘ç»œï¼Œå…è®¸é€‚åº”å¿ƒè„å›¾åƒ
            # åªå†»ç»“å‰å‡ å±‚ï¼Œè§£å†»åå‡ å±‚è®©å…¶é€‚åº”å¿ƒè„è¶…å£°å›¾åƒ
            total_blocks = len(self.sam_model.image_encoder.blocks)
            freeze_blocks = total_blocks // 2  # å†»ç»“å‰ä¸€åŠï¼Œè§£å†»åä¸€åŠ
            
            # é™é»˜è§£å†»SAM
            for param in self.sam_model.image_encoder.parameters():
                param.requires_grad = True
            
            if hasattr(self.sam_model.image_encoder, 'neck'):
                for param in self.sam_model.image_encoder.neck.parameters():
                    param.requires_grad = True
            
        except Exception as e:
            pass  # é™é»˜å¤„ç†é”™è¯¯
    
    def verify_weights_loaded(self):
        """éªŒè¯é¢„è®­ç»ƒæƒé‡æ˜¯å¦çœŸæ­£ç”Ÿæ•ˆ"""
        print("\nğŸ” éªŒè¯SAMé¢„è®­ç»ƒæƒé‡æ˜¯å¦çœŸæ­£ç”Ÿæ•ˆ...")
        
        # æµ‹è¯•ï¼šç›¸åŒè¾“å…¥ä¸¤æ¬¡å‰å‘ï¼Œç‰¹å¾æ˜¯å¦ä¸€è‡´ï¼ˆå†»ç»“ï¼‰
        x1 = torch.randn(1, 3, 1024, 1024)
        x2 = x1.clone()
        
        # ç¡®ä¿åœ¨è¯„ä¼°æ¨¡å¼
        self.sam_model.image_encoder.eval()
        
        with torch.no_grad():
            feat1 = self.sam_model.image_encoder(x1)
            feat2 = self.sam_model.image_encoder(x2)
        
        feature_diff = (feat1 - feat2).abs().max().item()
        print(f"ç‰¹å¾å·®å¼‚: {feature_diff:.10f} (åº”è¯¥æ¥è¿‘0)")
        
        if feature_diff < 1e-6:
            # é™é»˜åŠ è½½SAMé¢„è®­ç»ƒæƒé‡
            return True
        else:
            print("âŒ SAMé¢„è®­ç»ƒæƒé‡å¯èƒ½æœªç”Ÿæ•ˆï¼")
            return False
    
    def _interpolate_rel_pos(self, rel_pos, new_shape):
        """æ’å€¼ç›¸å¯¹ä½ç½®ç¼–ç """
        import torch.nn.functional as F
        
        old_len, c = rel_pos.shape
        new_len = new_shape[0]
        
        if old_len == new_len:
            return rel_pos
        
        # å°†2Dç›¸å¯¹ä½ç½®ç¼–ç è½¬æ¢ä¸º4Dè¿›è¡Œæ’å€¼
        rel_pos = rel_pos.unsqueeze(0).unsqueeze(0)  # [1, 1, L, C]
        rel_pos = F.interpolate(rel_pos, size=(new_len, c), mode='bilinear', align_corners=False)
        return rel_pos.squeeze(0).squeeze(0)  # [new_len, C]
